<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="BSc Geography" />


<title>Introduction to Remote Sensing</title>
<!-- Material Design fonts -->
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/icon?family=Material+Icons">
<script src="index_files/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="index_files/bootstrap-3.3.7/css/bootstrap.min.css" rel="stylesheet" />
<script src="index_files/bootstrap-3.3.7/js/bootstrap.min.js"></script>
<script src="index_files/jqueryui-1.11.4/jquery-ui.min.js"></script>
<script src="index_files/navigation-1.1/tabsets.js"></script>
<script src="index_files/navigation-1.1/codefolding.js"></script>
<link href="index_files/magnific-popup-1.1.0/magnific-popup.css" rel="stylesheet" />
<script src="index_files/magnific-popup-1.1.0/jquery.magnific-popup.min.js"></script>
<link href="index_files/bootstrap_material-0.1/bootstrap-material-design.min.css" rel="stylesheet" />
<link href="index_files/bootstrap_material-0.1/ripples.min.css" rel="stylesheet" />
<script src="index_files/bootstrap_material-0.1/material.min.js"></script>
<script src="index_files/bootstrap_material-0.1/ripples.min.js"></script>
<link href="index_files/material-0.1/material.css" rel="stylesheet" />
<script src="index_files/material-0.1/material.js"></script>
<script src="index_files/accessible-code-block-0.0.1/empty-anchor.js"></script>



<link rel="stylesheet" href="material_adjust_blue.css" type="text/css" />

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("toc");
});
</script>

<!-- code folding -->

</head>

<body>

<div class="header-panel shadow z-2">
    <div class="container-fluid">
        <div class="row">
            <div class="col-xs-3">
        <div id="header">
    <h1 class="title">Introduction to Remote Sensing</h1>
                <h4 class="author">BSc Geography</h4>
                <h4 class="date">Winter term 2020/2021</h4>
        </div>
    </div>
</div>
</div>
</div>


<div class="container-fluid main-container">
    <div class="row">
      <nav class="col-xs-3 menu">
        <div id="toc">
        <ul>
        <li><a href="#welcome">Welcome!</a></li>
        <li><a href="#course-materials">Course materials</a></li>
        <li><a href="#visual-image-interpretation">Visual image interpretation</a></li>
        <li><a href="#on-screen-visualization">On-screen visualization</a></li>
        <li><a href="#land-cover-land-use-classification">Land cover / land use classification</a></li>
        <li><a href="#lab-and-field-spectroscopy">Lab and field spectroscopy</a></li>
        <li><a href="#optical-data-multi--hyperspectral">Optical data (Multi- / Hyperspectral)</a></li>
        <li><a href="#enmap-box-for-qgis">EnMAP Box for QGIS</a></li>
        <li><a href="#data-acquisition">Data acquisition</a></li>
        <li><a href="#vegetation-properties-spectral-indices">Vegetation properties &amp; spectral indices</a></li>
        <li><a href="#image-classification">Image classification</a></li>
        <li><a href="#random-forest">Random forest</a></li>
        <li><a href="#accuracy-assessment">Accuracy assessment</a></li>
        <li><a href="#sentinel-2-time-series">Sentinel-2 time series</a></li>
        <li><a href="#modis-time-series">MODIS time series</a></li>
        <li><a href="#case-studies-change-detection">Case studies: Change detection</a></li>
        </ul>
        </div>
        
        
        
      </nav>
     <div class="pages col-xs-9">
     <div class="row">
       <div class="col-xs-10">



<div id="welcome" class="section level1">
<h1>Welcome!</h1>
<div class="figure">
<img src="fig/EO_header.PNG" alt="" />
<p class="caption">EOL Logo</p>
</div>
<div id="about" class="section level2">
<h2>About</h2>
<p>Introduction to Remote Sensing is an introductory remote sensing course for Geography students at Humboldt-Universität zu Berlin. In this course, you will be exposed to theoretical fundaments and introductory applications of remote sensing. The course is based on open source software.</p>
<hr />
</div>
<div id="requirements" class="section level2">
<h2>Requirements</h2>
<hr />
</div>
<div id="learning-goals-course-contents" class="section level2">
<h2>Learning goals &amp; course contents</h2>
<hr />
</div>
</div>
<div id="course-materials" class="section level1">
<h1>Course materials</h1>
<div id="software" class="section level2">
<h2>Software</h2>
<p>We use openly available and platform independent (Windows, Linux, Mac OS) software packages throughout this course. Please install the latest versions of:</p>
<ul>
<li><a href="https://www.r-project.org/">R v3.6.X</a></li>
<li><a href="https://rstudio.com/products/rstudio/">R studio v1.2.X</a></li>
<li><a href="https://www.qgis.org/">QGIS v3.1X.X</a></li>
</ul>
<hr />
</div>
<div id="data" class="section level2">
<h2>Data</h2>
<hr />
</div>
<div id="assignments" class="section level2">
<h2>Assignments</h2>
<p>The weekly assignments are defined in the respective session. Each session comprises several tasks that involve scipting in R. Course participants must submit completed assignments, documented as R scripts, in <a href="http://moodle.hu-berlin.de/">moodle</a> to pass. Weekly submission deadlines are monday, 23:59. Please name the script of your work group as SXX_name1_name2.R. Please structure your script for every assignment as follows:</p>
<hr />
</div>
</div>
<div id="visual-image-interpretation" class="section level1">
<h1>Visual image interpretation</h1>
<div class="figure">
<img src="fig/Berlin_2009.png" alt="" />
<p class="caption">Berlin from above in Google Earth<sup>TM</sup>, 2009</p>
</div>
<div class="figure">
<img src="fig/Berlin_2019.png" alt="" />
<p class="caption">Berlin from above in Google Earth<sup>TM</sup>, 2019</p>
</div>
<div id="image-interpretation" class="section level2">
<h2>Image interpretation</h2>
<ul>
<li><p>Open Google Earth<sup>TM</sup></p></li>
<li><p>Deactivate the oblique view (use key “R”)</p></li>
<li><p>Disable multimedia (e.g. pictures)</p></li>
<li><p>Move to two different positions in Berlin and answer the following questions with help of the aerial photographs:</p>
<ul>
<li><p>Which season was the flight operated in?</p></li>
<li><p>Is it possible to get the exact month and/or day?</p></li>
<li><p>What was the day of the week?</p></li>
<li><p>What was the time of day?</p></li>
</ul></li>
</ul>
</div>
<div id="digitizing" class="section level2">
<h2>Digitizing</h2>
<ul>
<li><p>Navigate to Campus Adlershof</p></li>
<li><p>Right-click on ‘Meine Orte’ and create a new folder called ‘FE1’</p></li>
<li><p>Use the digitizing tools (see image below) to…</p>
<ul>
<li><p>Mark the wind tunnel as a point</p></li>
<li><p>Digitize a section of the S-Bahn trail as a line</p></li>
<li><p>Save the Institute of Geography as a polygon (semitransparent and outlined)</p></li>
</ul></li>
<li><p>The results of the digitization need to be located in the ‘FE1’ folder</p></li>
<li><p>Save the folder ‘FE1’ with a right-click on the folder as a .kmz file on O:/…</p>
<div class="figure">
<img src="fig/digitizing_tools.png" alt="" />
<p class="caption">Digitizing tools</p>
</div></li>
</ul>
</div>
<div id="observing-change" class="section level2">
<h2>Observing change</h2>
<ul>
<li><p>Use the date tool (see image below) to observe older/historical photos of Adlershof and answer the following questions:</p>
<ul>
<li><p>What is the frequency of photo observations before and after 2010?</p></li>
<li><p>What major changes can be detected in Adlershof in 2000, 2010 and 2019?</p></li>
<li><p>Do your polygons/lines/points fit older pictures as well?</p></li>
<li><p>Describe the differences of image data in 1953, 2000 and 2019. Why do they differ?</p></li>
</ul>
<div class="figure">
<img src="fig/datumstool.png" alt="" />
<p class="caption">Date tool</p>
</div></li>
</ul>
</div>
<div id="summary" class="section level2">
<h2>Summary</h2>
<ul>
<li><p>The analysis of earth observation data (satellite and aerial images) allows us to draw a variety of conclusions about processes and conditions of the Earth’s surface. For such analysis, different image properties are used (object features and context).</p></li>
<li><p>With help of image data from different dates, changes of the Earth’s surface can be analyzed.</p></li>
<li><p>The quality of image data enhanced over time. Today, satellite images with a spatial resolution of less than 1 m are available for most parts of the earth, in urban agglomerations aerial photographs often even exceed 10 cm spatial resolution.</p></li>
</ul>
</div>
<div id="exercise" class="section level2">
<h2>Exercise</h2>
<p>Characterize the land cover and land use change in Berlin with Google Earth<sup>TM</sup>.</p>
<ul>
<li><p>On O:/WS2021_FE1/S01 you find a .kmz file with two given areas (Berlin-Mitte and Berlin-Adlershof)</p></li>
<li><p>Choose three suitable objects within each area and outline them with the digitizing tools</p></li>
<li><p>Choose three time steps (depending on the available data) that show a change process for your chosen objects</p></li>
<li><p>Answer the following questions for your objects (with images and text):</p>
<ul>
<li><p>What changes contentwise?</p></li>
<li><p>What changes concretely in the photos?</p></li>
</ul></li>
<li><p><strong>Assignment:</strong> Upload your results as a PDF file on Moodle (each participant separately).</p></li>
</ul>
<hr />
</div>
</div>
<div id="on-screen-visualization" class="section level1">
<h1>On-screen visualization</h1>
<div id="recap" class="section level2">
<h2>Recap</h2>
<ul>
<li><p>We can describe the human eye as a ‘sensor with three bands’</p></li>
<li><p>Sensitive for electromagnetic radiation (EMR) in the blue, green and red regions: <strong>spectral range</strong> between ~400-700 nm</p></li>
<li><p>The receptors for the three colors are stimulated across wavelength regions of ~150-200 nm (<strong>spectral resolution</strong>)</p></li>
<li><p>The intervals between the wavelengths of maximum sensitivity are ~50 nm to 150 nm wide (<strong>spectral sampling interval</strong>)</p></li>
</ul>
<div class="figure">
<img src="fig/retinal%20response.png" alt="" />
<p class="caption">Retinal response of the human eye (Source: <a href="http://www.planetary.org/rrgtm/emspectrum.html">planetary.org LINK ÜBERPRÜFEN!</a> )</p>
</div>
<ul>
<li>Digital cameras correspond to imaging sensors with three bands</li>
</ul>
<div class="figure">
<img src="fig/red-green-blue.png" alt="" />
<p class="caption">Red-green-blue representation</p>
</div>
<ul>
<li><p>Spectrometers are sensitive to wavelengths beyond the human eye’s sensitivity</p></li>
<li><p><strong>Optical remote sensing</strong> makes use of the visible light (~380 - 700 nm), and the near and short-wave infrared (0.7 - ~3 µm)</p></li>
<li><p><strong>Thermal remote sensing</strong> detects thermal infrared radiation (5 - 15 µm)</p></li>
<li><p><strong>Radar remote sensing</strong> detects microwave radiation (1 mm - 1 m)</p></li>
<li><p>Common abbrevations:</p>
<ul>
<li><p>VIS = visible: 380 - 700 nm</p></li>
<li><p>nIR = near infrared: 0.7 - 1.3 µm</p></li>
<li><p>swIR = short wave infrared: 1.3 - 3 µm</p></li>
<li><p>tIR = thermal infrared: 5 - 15 µm</p></li>
</ul></li>
</ul>
<div class="figure">
<img src="fig/wavelength%20regions.jpg" alt="" />
<p class="caption">Wavelength regions (Source: <a href="http://www.arm.gov/news/facility/post/5946">arm.gov LINK ÜBERPRÜFEN!</a>)</p>
</div>
<ul>
<li>Optical remote sensing sensors make it possible to take images in the visible light (VIS), near-infrared (nIR) and shortwave-infrared (swIR)</li>
</ul>
<div class="figure">
<img src="fig/Bild%20sichtbares%20Licht.png" alt="" />
<p class="caption">Image showing the reflection of the visible light</p>
</div>
<div class="figure">
<img src="fig/Bild%20nir%20swir.png" alt="" />
<p class="caption">Image showing the reflection of the near-infrared and visible light</p>
</div>
</div>
<div id="additive-color-model" class="section level2">
<h2>Additive color model</h2>
<ul>
<li><p>Primary colours: red, green and blue (RGB)</p></li>
<li><p>Values range from 0 to 255 (the higher the more intense)</p></li>
<li><p>Max. 256 * 256 * 256 = 16,7 mio colours</p></li>
<li><p>Complementary/secondary colours: cyan, magenta and yellow (CMY)</p></li>
</ul>
<div class="figure">
<img src="fig/RGB%20Farbmodell.png" alt="" />
<p class="caption">RGB color model (Source left: <a href="http://www.orange-sinne.de/additives_farbmodell.html">orange-sinne.de</a>, source right: <a href="http://www.informatikzentrale.de">informatikzentrale.de</a>)</p>
</div>
<div id="exercise-for-colour-mixing-in-qgis" class="section level3">
<h3>Exercise for colour mixing in QGIS</h3>
<ul>
<li><p>Open the shapefile ‘charlottenburg_point.shp’ in a new QGIS-project</p></li>
<li><p>Open the ‘Select Color Tool’ of the shapefile (Properties &gt; Symbology &gt; double click on colour bar)</p></li>
<li><p>For displaying different colours QGIS offers the RGB and HSV colour model</p></li>
</ul>
<div class="figure">
<img src="fig/RGB%20QGIS.png" alt="" />
<p class="caption">Select Color Tool in QGIS</p>
</div>
<ul>
<li><p>Enter the colour values of the following table in the QGIS colour model and note/describe the resulting colour</p>
<table>
<thead>
<tr class="header">
<th align="center">red</th>
<th align="center">green</th>
<th align="center">blue</th>
<th align="center">colour</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">255</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">255</td>
<td align="center">0</td>
<td align="center">255</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center">255</td>
<td align="center">255</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">255</td>
<td align="center">255</td>
<td align="center">0</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">0</td>
<td align="center">0</td>
<td align="center">0</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">25</td>
<td align="center">25</td>
<td align="center">25</td>
<td align="center"></td>
</tr>
<tr class="odd">
<td align="center">150</td>
<td align="center">150</td>
<td align="center">150</td>
<td align="center"></td>
</tr>
<tr class="even">
<td align="center">255</td>
<td align="center">255</td>
<td align="center">255</td>
<td align="center"></td>
</tr>
</tbody>
</table></li>
</ul>
</div>
</div>
<div id="screen-representation-of-remote-sensing-images" class="section level2">
<h2>Screen representation of remote sensing images</h2>
<ul>
<li>Remote sensing images can be presented as a greyscale image (single band) or RGB-composite (combination of three different bands)</li>
</ul>
<div class="figure">
<img src="fig/rgb%20fe%20bilder.png" alt="" />
<p class="caption">Satellite images as single band grey, RGB-composite (true colour), RGB-composite (false colour)</p>
</div>
</div>
<div id="image-histogram" class="section level2">
<h2>Image histogram</h2>
<ul>
<li>Image histograms show the frequency distribution of pixel values i.e. of a single band</li>
</ul>
<div class="figure">
<img src="fig/histogramm%201.png" alt="" />
<p class="caption">Band 3 (red) as a greyscale image and its image histogram</p>
</div>
<div id="contrast-stretch" class="section level3">
<h3>Contrast stretch</h3>
<ul>
<li><p>Every RGB-channel has a colour depth of 8-bit on a monitor (equivalent to 256 greyscales)</p></li>
<li><p>Transfer of pixel values of an image (or band) to a monitor initially 1:1</p></li>
<li><p>At best, full grey value range is utilized optimally</p></li>
</ul>
<div class="figure">
<img src="fig/histogramm%202.png" alt="" />
<p class="caption">Image histogram and satellite image with an optimally utilized grey value range</p>
</div>
<ul>
<li><p>Due to recording and sensor conditions, data sets (or bands) often display only a section of the 256 obtainable grey values</p></li>
<li><p>With a 1:1 transfer, images are often low-contrast</p></li>
</ul>
<div class="figure">
<img src="fig/histogramm%203.png" alt="" />
<p class="caption">Image histogram and satellite image with low-contrast</p>
</div>
<ul>
<li><p>“Stretching” the image histogram, the screen display can be enhanced</p></li>
<li><p>Caution! The screen display changes, the data stays the same!</p></li>
</ul>
<div class="figure">
<img src="fig/histogramm%204.png" alt="" />
<p class="caption">Stretching of an image histogram</p>
</div>
<ul>
<li><p>Contrast enhancement describes a function of representation that is used to transfer pixel values in grey values</p></li>
<li><p>Often, linear contrast enhancement is used where the increase in gray value per increase in pixel value remains the same for the relevant value ranges</p></li>
</ul>
<div class="figure">
<img src="fig/histogramm%205.png" alt="" />
<p class="caption">Min-max linear contrast stretch and standard deviation linear contrast stretch (Source: Jensen, 2011)</p>
</div>
</div>
</div>
<div id="exercise-1" class="section level2">
<h2>Exercise</h2>
<ul>
<li><p>Open the Sentinel-2 image ‘20150704_LEVEL2_SEN2A_BOA_berlin.bsq’ in QGIS</p></li>
<li><p>The image has a spatial resolution of 10 m, an extent of 2700 x 2700 pixel and 4 spectral bands (B1 = blue, B2 = green, B3 = red, B4 = nIR)</p></li>
<li><p>Open the display options of the image (Symbology)</p></li>
<li><p>What are the default settings after loading the image:</p>
<ul>
<li><p>render type</p></li>
<li><p>RGB assignment</p></li>
<li><p>min/max value settings</p></li>
</ul></li>
</ul>
<div class="figure">
<img src="fig/bildschirmdarstellung%20qgis.png" alt="" />
<p class="caption">Layer properties &gt; Symbology in QGIS</p>
</div>
<ul>
<li>Display the Sentinel-2 image in true colours</li>
</ul>
<div class="figure">
<img src="fig/sen2%20echtfarben.png" alt="" />
<p class="caption">Sentinel-2 image in true colours</p>
</div>
<ul>
<li><p>Now, display only band 1 (blue) in greyscale (Render type &gt; Singleband grey)</p>
<ul>
<li><p>How did the min/max value settings change?</p></li>
<li><p>How rich in contrast does the image appear?</p></li>
<li><p>How does the image representation change when you alter the min/max value settings (i.e. cumulative count cut, etc.)?</p></li>
</ul></li>
</ul>
<ol style="list-style-type: decimal">
<li><p>Switch to the histogram section and calculate the image histogram</p></li>
<li><p>Adjust the settings as shown in the picture below</p></li>
<li><p>Select the nIR band</p></li>
<li><p>Vary the settings for min/max values to alter the contrast manually</p></li>
<li><p>Try to differentiate water bodies from land mass by stretching the min/max values</p></li>
</ol>
<div class="figure">
<img src="fig/bildschirmdarstellung%20qgis%202.png" alt="" />
<p class="caption">Settings for image histogram</p>
</div>
<ol style="list-style-type: decimal">
<li><p>Create a true and false colour (R = nIR, G = red, B = green) representation of the Sentinel 2 image and describe the main differences (key words)</p></li>
<li><p>Find one example (Screenshot) for each of the following surfaces with the true and false colour representation:</p>
<ul>
<li><p>Deciduous forest</p></li>
<li><p>Coniferous forest</p></li>
<li><p>Artificial turf pitch</p></li>
<li><p>True turf pitch</p></li>
</ul></li>
<li><p>Subsequently, describe the differences between deciduous and coniferous forest and artificial and true turf</p></li>
<li><p>For what phenomenons and/or surfaces is the nIR channel particularly sensitive?</p></li>
</ol>
<ul>
<li><strong>Assignment:</strong> Summarize task 1 - 4 visually and textually and upload your results as a PDF file on Moodle.</li>
</ul>
<hr />
</div>
</div>
<div id="land-cover-land-use-classification" class="section level1">
<h1>Land cover / land use classification</h1>
<div id="mapping-the-earths-surface" class="section level2">
<h2>Mapping the Earth’s surface</h2>
<ul>
<li><p>The Earth’s surface is a mosaic of various landscapes (natural, semi-natural and anthropogenic)</p></li>
<li><p>Mapping with remote sensing requires generalized/transferable classification schemes to describe the Earth’s surface</p></li>
</ul>
<div class="figure">
<img src="fig/landscape.png" alt="" />
<p class="caption">Various landscapes including natural, semi-natural and anthropogenic surface types</p>
</div>
</div>
<div id="land-cover-or-land-use" class="section level2">
<h2>Land cover or land use?</h2>
<ul>
<li>What does a satellite see from orbit?</li>
</ul>
<div class="figure">
<img src="fig/Blick%20Satellit%201.jpg" alt="" />
<p class="caption">View from satellite platform</p>
</div>
<div class="figure">
<img src="fig/Blick%20Satellit%202.jpg" alt="" />
<p class="caption">Spatial resolution affects what the satellite records</p>
</div>
<ul>
<li><p>Land <strong>cover</strong>:</p>
<ul>
<li><p>Biophysical cover of the Earth’s surface</p></li>
<li><p>For instance crops, forest, build-up area</p></li>
</ul></li>
<li><p>Land <strong>use</strong>:</p>
<ul>
<li><p>Anthropogenic use of the Earth’s surface</p></li>
<li><p>For instance agriculture, forestry, residential area</p></li>
</ul></li>
</ul>
<div class="figure">
<img src="fig/lu%20lc%201.png" alt="" />
<p class="caption"><strong>Land cover</strong>: crops, <strong>land use</strong>: agriculture (left); <strong>land cover</strong>: forest, <strong>land use</strong>: forest management (middle); <strong>land cover</strong>: build-up area, <strong>land use</strong>: residential area (right)</p>
</div>
</div>
<div id="visual-image-interpretation-1" class="section level2">
<h2>Visual image interpretation</h2>
<table>
<colgroup>
<col width="26%" />
<col width="43%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">factor</th>
<th align="left">explanation</th>
<th align="left">example</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">contrast, colour, brightness</td>
<td align="left">depend on reflective properties of the recorded surfaces</td>
<td align="left">depth of water, roof material</td>
</tr>
<tr class="even">
<td align="left">shape and size</td>
<td align="left">geometry, outlines and edges that divide the landscape in different objects</td>
<td align="left">airport, river course</td>
</tr>
<tr class="odd">
<td align="left">texture</td>
<td align="left">structure of a surface due to material or surface properties, strongly depends on scale</td>
<td align="left">maize cultivation, fields, forest</td>
</tr>
<tr class="even">
<td align="left">spatial context</td>
<td align="left">determines functional interrelationship i.e. building function</td>
<td align="left">railway station, lido, farm</td>
</tr>
<tr class="odd">
<td align="left">shadows</td>
<td align="left">contain information on object form, height and function</td>
<td align="left">wind turbine, landscape topography</td>
</tr>
</tbody>
</table>
<div class="figure">
<img src="fig/lu%20lc%202.png" alt="" />
<p class="caption"><strong>Land cover</strong>: crops and bare soil, <strong>land use</strong>: agriculture (upper left); <strong>land cover</strong>: pines, <strong>land use</strong>: forestry (upper right); <strong>land cover</strong>: build-up area, <strong>land use</strong>: city district (bottom left); <strong>land cover</strong>: build-up area, river and grassland, <strong>land use</strong>: village (bottom right)</p>
</div>
</div>
<div id="land-use-cover-area-frame-survey-lucas" class="section level2">
<h2>Land use / cover area frame survey (LUCAS)</h2>
<ul>
<li><p>Sample survey for land use and land cover in the EU</p>
<ul>
<li><p>Collection of data on land use and land cover (LULC) as well as temporal changes</p></li>
<li><p>In situ gathering of point data by a standardized classification scheme to ensure comparability between EU member states</p></li>
</ul></li>
<li><p>Used in the context of</p>
<ul>
<li><p>Agriculture policy i.e. common agriculture policy</p></li>
<li><p>Conservation i.e. EU biodiversity strategy</p></li>
<li><p>Environmental monitoring i.e. COPERNICUS</p></li>
</ul></li>
<li><p><a href="https://ec.europa.eu/eurostat/statistics-explained/index.php/LUCAS_-_Land_use_and_land_cover_survey"><strong>Further information on LUCAS</strong></a></p></li>
<li><p>LUCAS points are based on a 2-km-raster (ca. 1,1 mio points) from which around 270,000 were extracted for the survey</p></li>
<li><p>Survey takes place every three years (last 2018) involving ca. 750 cartographers</p></li>
</ul>
<div class="figure">
<img src="fig/LUCAS%201.png" alt="" />
<p class="caption">Points used for the LUCAS survey in Berlin and Northern Brandenburg (Source: <a href="http://ec.europa.eu/eurostat/statistical-atlas/gis/viewer/?config=LUCAS-2015.json">eurostat</a>)</p>
</div>
<div class="figure">
<img src="fig/LUCAS%202.png" alt="" />
<p class="caption">Land cover of the EU-27 (Source: EU-LUCAS 2012)</p>
</div>
<ul>
<li>LUCAS as reference for remote sensing analyses i.e. europe-wide classification of land cover (Pflugmacher et. al 2018)</li>
</ul>
<div class="figure">
<img src="fig/LUCAS%203.png" alt="" />
<p class="caption">Classification of land cover of the EU member states in 2018 (Source: <a href="https://www2.hu-berlin.de/geovis/eolab/europeanlandcover/euroLandCover.html">HU-Berlin geovis</a>)</p>
</div>
<div id="lucas-classification-scheme" class="section level3">
<h3>LUCAS classification scheme</h3>
<div class="figure">
<img src="fig/LUCAS%204.png" alt="" />
<p class="caption">LUCAS classification scheme divided into land cover (green) and land use (red)</p>
</div>
</div>
</div>
<div id="exercise-2" class="section level2">
<h2>Exercise</h2>
<p><strong>Part 1: Development of a classification scheme based on LUCAS nomenclature</strong></p>
<ul>
<li><p>Open the false-colour satellite image of Wuhlheide (1990) on O:/ in QGIS</p>
<div class="figure">
<img src="fig/bild%20uebung%20S03.png" alt="" />
<p class="caption">False-colour satellite image of Wuhlheide, Berlin, from 1990</p>
</div></li>
<li><p>Create a classification scheme for the following nine surface types:</p>
<ul>
<li>Buildings</li>
<li>Streets</li>
<li>Rails</li>
<li>Sealed (non-build-up) areas</li>
<li>Coniferous forest</li>
<li>Deciduous forest</li>
<li>Mixed forest</li>
<li>Grassland</li>
<li>Water</li>
</ul></li>
<li><p>Add a sample image and a short description (class properties) for every class</p></li>
<li><p>Further, add the LUCAS land use and land cover class. Use the most detailed level! (See <em>LUCAS2015_C1_Annex_Classification_sheme.pdf</em>)</p></li>
</ul>
<div class="figure">
<img src="fig/exercise%20S03.png" alt="" />
<p class="caption">Example for surface type ‘building’</p>
</div>
<p><strong>Part 2: Mapping of your example areas</strong></p>
<ul>
<li><p>Digitize your example areas in QGIS according to your classification scheme from task 1. Note the following:</p>
<ul>
<li><p>Three example areas for each class</p></li>
<li><p>Minimum mapping unit equals 0.05 ha (500 m<sup>2</sup>), objects smaller than that are included in the surrounding class</p></li>
</ul></li>
<li><p>Create a map with your results and add a legend</p></li>
<li><p>Note and discuss problems and inaccuracies (key words)</p></li>
<li><p><strong>Assignment:</strong> Upload the classififation scheme, map with legend and your discussion as a PDF file on Moodle.</p></li>
</ul>
<hr />
</div>
</div>
<div id="lab-and-field-spectroscopy" class="section level1">
<h1>Lab and field spectroscopy</h1>
<div id="the-spectral-dimension-of-remote-sensing-data" class="section level2">
<h2>The spectral dimension of remote sensing data</h2>
<ul>
<li>How do we produce such measurements that allow differentiating different surfaces or monitoring changes of surfaces over time?</li>
</ul>
<div class="figure">
<img src="fig/arena.png" alt="" />
<p class="caption">True colour image (left) and false colour image (right) of the stadium on the Wisconsin-Madison University campus, USA (Source: Lillesand, Kiefer and Chipman 2008)</p>
</div>
<div class="figure">
<img src="fig/spectroscopy%20arena.png" alt="" />
<p class="caption">Reflectance of natural and artificial turf (Source: Lillesand, Kiefer and Chipman 2008)</p>
</div>
</div>
<div id="basics---what-is-electromagnetic-radiance-what-do-we-measure" class="section level2">
<h2>Basics - What is electromagnetic radiance? What do we measure?</h2>
<div id="the-nature-of-electromagnetic-energy" class="section level3">
<h3>The nature of electromagnetic energy</h3>
<ul>
<li><p>Energy (or light) travels as a periodic electromagnetic field (sinusoidal wave)</p></li>
<li><p>Light travels with – surprise! – <strong>speed of light <em>(c)</em> </strong></p></li>
<li><p>Relevant characteristics are <strong>wavelength <em>λ</em> [nm]</strong> and <strong>frequency <em>ν</em> [Hz]</strong></p></li>
<li><p>The wavelength is hence defined according to: <span class="math inline">\(λ = c / ν\)</span></p></li>
<li><p>In remote sensing, wavelength is the common unit</p>
<ul>
<li><p>1 nm (nanometer) = 1 x 10<sup>-9</sup> m</p></li>
<li><p>1 µm (micrometer) = 1 x 10<sup>-6</sup> m</p></li>
<li><p>1 mm (millimeter) = 1 x 10<sup>-3</sup> m</p></li>
</ul></li>
<li><p>In remote sensing, a target’s spectral properties are measured across a wavelength region and in several or even many distinct bands</p></li>
</ul>
<div class="figure">
<img src="fig/electromagnetic%20radiance.png" alt="" />
<p class="caption">Electromagnetic wavelength (Source: Drury 1993)</p>
</div>
</div>
<div id="electromagnetic-spectrum" class="section level3">
<h3>Electromagnetic spectrum</h3>
<ul>
<li><p>Spectrometers are sensitive to wavelengths beyond the human eye’s sensitivity</p></li>
<li><p>Optical remote sensing makes use of the visible light (~400 - 700 nm), and the near and short-wave infrared (0,7 - ~3 µm)</p></li>
<li><p>Thermal remote sensing detects thermal infrared radiation (5 - 15 µm)</p></li>
<li><p>Radar remote sensing detects microwave radiation (1 mm - 1 m)</p></li>
<li><p>Common abbrevations:</p>
<ul>
<li><p>VIS = visible: 370 - 700 nm</p></li>
<li><p>nIR = near infrared: 0.7 - 1.3 µm</p></li>
<li><p>swIR = short wave infrared: 1.3 - 3 µm</p></li>
<li><p>tIR = thermal infrared: 5 - 15 µm</p></li>
</ul></li>
</ul>
<div class="figure">
<img src="fig/wavelength%20regions.jpg" alt="" />
<p class="caption">Wavelength regions (Source: <a href="http://www.arm.gov/news/facility/post/5946">arm.gov LINK ÜBERPRÜFEN!</a>)</p>
</div>
</div>
<div id="measuring-surface-properties" class="section level3">
<h3>Measuring surface properties</h3>
<ul>
<li><p>Geo- and environmental sciences are primarily interested in processes at the Earth’s surface</p></li>
<li><p>From the interaction between radiation and surface we can draw conclusions on the surface’s characteristics</p></li>
<li><p>Each surface’s interaction with radiation depends on it’s physical and chemical properties and varies with wavelength</p></li>
<li><p>That’s the <strong>basis for the entire remote sensing process</strong></p></li>
<li><p>To start with: let’s control illumination and ignore the atmosphere</p></li>
<li><p>Let’s assume laboratory conditions (using an artificial light source instead of the sun)</p></li>
</ul>
<div class="figure">
<img src="fig/Kraus%20und%20Schneider.png" alt="" />
<p class="caption">Remote sensing without atmosphere (Source: Kraus und Schneider 1988)</p>
</div>
</div>
<div id="a-laboratory-setup" class="section level3">
<h3>A laboratory setup</h3>
<ul>
<li><p>Lab measurements are performed with a spectrometer and allow an exact characterization of a surface</p></li>
<li><p>We control illumination by using a lamp</p></li>
<li><p>We control the distance and angle to the surface to be measured (usually nadir measurement)</p></li>
<li><p>We control the instrument calibration, i.e. we measure and eliminate the noise from the signal</p></li>
</ul>
<div class="figure">
<img src="fig/laboratory%20setup.jpg" alt="" />
<p class="caption">Typical laboratory setup with an ASD spectrometer (in blue), the fiber optics ending in a pistol grip mounted on a tripod and a video spot as artificial illumination source. The bright target on the smaller tripod is a calibration panel. (Source: <a href="http://www.lineas.cchs.csic.es/biospec/methods/laboratory">Biospec</a>)</p>
</div>
</div>
<div id="measuring-target-radiance" class="section level3">
<h3>Measuring target radiance</h3>
<ul>
<li><p>In Earth remote sensing, we perform measurements from above, i.e. we are interested in the reflected portion of the energy</p></li>
<li><p>At any surface, incoming radiation is either partially or fully:</p>
<ul>
<li><p>reflected (ρ)</p></li>
<li><p>absorbed (α)</p></li>
<li><p>transmitted (τ)</p></li>
</ul></li>
<li><p>The sum of all three terms is always 100% of the incoming radiance: <span class="math inline">\(ρ + α + τ = 1\)</span> (Law of conservation of energy)</p></li>
</ul>
<div class="figure">
<img src="fig/light%20and%20surface.png" alt="" />
<p class="caption">Emittance, absorbance, reflectance and transmittance of energy (light) (Source: <a href="http://www.edaphic.com.au/emittance-absorbance-reflectance-transmittance/">edaphic.com</a>)</p>
</div>
<ul>
<li><p>The remote sensing sensor measures the reflected portion of the energy in the physical quantity called <strong>radiance</strong></p></li>
<li><p>The unit of radiance is watts per square meter per steradian: W • m<sup>-2</sup> • sr<sup>-1</sup></p></li>
</ul>
<blockquote>
<p><em>The energy (i.e. electro magnetic radiation [W]) that is reflected (or emitted/transmitted) by a unit of a surface [per m²] into a portion of the hemisphere [per sr]?</em></p>
</blockquote>
</div>
<div id="excursus---solid-angle" class="section level3">
<h3>Excursus - solid angle</h3>
<ul>
<li><p>A three-dimensional angle is called <strong>solid angle Ω</strong></p></li>
<li><p>Definition Ω: the area A cut out from a sphere with the radius r</p></li>
<li><p>Unit: <strong>steradian [sr]</strong></p></li>
<li><p>Ω = 1 sr if a conus cuts an area A of 1 m<sup>2</sup> from a sphere with the radius r = 1 m</p></li>
</ul>
<div class="figure">
<img src="fig/Raumwinkel.png" alt="" />
<p class="caption">Solid angle Ω (Source: <a href="https://de.wikipedia.org/wiki/Raumwinkel">Wikipedia</a>)</p>
</div>
</div>
<div id="from-radiance-to-reflectance" class="section level3">
<h3>From radiance to reflectance</h3>
<ul>
<li><p>The problem with radiance as the unit of measure: it varies with illumination</p></li>
<li><p>Needed: a stable measurement over time, independent of illumination, to avoid introducing changes when measuring unchanged surfaces</p></li>
<li><p>Solution: normalize the measurement relative to the irradiation</p></li>
<li><p>The ratio between radiance reflected at the Earth’s surface and the incoming radiation is called <strong>reflectance</strong></p>
<p>Reflectance = Rad<sub>reflected</sub> / Rad<sub>incoming</sub></p></li>
<li><p>Reflectance is the percentage of the total measurable radiation, which has not been absorbed or transmitted</p></li>
</ul>
<div class="figure">
<img src="fig/light%20and%20surface.png" alt="" />
<p class="caption">Emittance, absorbance, reflectance and transmittance of energy (light) (Source: <a href="http://www.edaphic.com.au/emittance-absorbance-reflectance-transmittance/">edaphic.com</a>)</p>
</div>
</div>
<div id="diffuse-and-direct-reflectance" class="section level3">
<h3>Diffuse and direct reflectance</h3>
<ul>
<li><p>The observation direction influences the measured signal</p></li>
<li><p>In most cases, surfaces reflect the incoming radiation diffusely</p></li>
<li><p>A special case is <strong>Lambert reflectance</strong> that is often assumed in remote sensing for the sake of simplicity</p></li>
<li><p>Lambert reflectance: the measured signal is independent of the observation direction (no directionality effects in the reflected signal)</p></li>
<li><p>In reality, most surfaces reflect in a mixed way, which can be described by the so called <strong>Bidirectional Reflectance Distribution Function (BRDF)</strong></p></li>
<li><p>Spectral reference panels (e.g. Spectralon®) are almost Lambert reflectors</p></li>
</ul>
<div class="figure">
<img src="fig/diffuse%20und%20direkte%20reflexion.png" alt="" />
<p class="caption">Direct reflectance (a), Lambertian reflectance (b) and mixed reflectance (c) (Source: Albertz 2001)</p>
</div>
</div>
</div>
<div id="spectral-surface-characteristics" class="section level2">
<h2>Spectral surface characteristics</h2>
<div class="figure">
<img src="fig/ENVI%20plot.png" alt="" />
<p class="caption">Example of a vegetation measurement (green) in the field</p>
</div>
<div class="figure">
<img src="fig/veg%20measurement%201.png" alt="" />
<p class="caption">Field measurements</p>
</div>
<div id="white-reference" class="section level3">
<h3>White reference</h3>
<ul>
<li>In addition to the target surface, a white surface is measured</li>
</ul>
<p>*This so-called White Reference is a surface with a reflectance of almost 100% across the entire wavelength spectrum</p>
<ul>
<li><p>Spectralon®, alternatively BaSO<sub>4</sub></p></li>
<li><p>Reference panels allow measuring the maximum backscatter for a particular point in time and position on Earth</p></li>
</ul>
<p>*If a panel reflects 100% across the spectral region of interest, its radiance is the same as the incoming radiance, i.e. it can serve for normalizing the reflectance</p>
<div class="figure">
<img src="fig/ENVI%20plot.png" alt="" />
<p class="caption">Example of a white reference measurement (red) and vegetetation measurement (green) in the field</p>
</div>
<div class="figure">
<img src="fig/white%20reference%20with%20veg.jpg" alt="" />
<p class="caption">White reference measurement in the field</p>
</div>
</div>
<div id="reflectance" class="section level3">
<h3>Reflectance</h3>
<ul>
<li><p>The ratio between the target radiance measured and the reference panel (white reference) is hence <strong>reflectance</strong></p>
<p>reflectance = Rad<sub>target</sub> / Rad<sub>WR</sub></p></li>
<li><p>Reflectance is the percentage of the total measurable radiation, which has not been absorbed (or transmitted)</p></li>
<li><p>Note: The division by Rad<sub>WR</sub> can result in considerable noise in wavelength regions with poor signal-to-noise-ratio</p></li>
</ul>
<div class="figure">
<img src="fig/reflectance%20spectra.png" alt="" />
<p class="caption">White reference spectra in red and vegetation spectra in green (left) and the resulting reflectance in green (right)</p>
</div>
</div>
</div>
<div id="reflectance-of-different-surfaces-and-materials" class="section level2">
<h2>Reflectance of different surfaces and materials</h2>
<div class="figure">
<img src="fig/ASD%20field%20measurements.png" alt="" />
<p class="caption">Field measurements</p>
</div>
<ul>
<li><p>The object-specific influence determines object-specific patterns in the spectrum of recorded electromagnetic radiation</p></li>
<li><p>The change of irradiation at the Earth’s surface results from material-specific absorption</p></li>
<li><p>We distinguish between absorption features due to electron transitions, molecular vibrations and molecular rotation</p></li>
<li><p>Electron transitions require a high amount of energy &gt; they occur mostly in the VIS</p></li>
<li><p>Vibration and rotation can lead to narrow absorption bands also in the nIR and SWIR</p></li>
</ul>
<div id="reflectance-of-photosynthetic-active-vegetation" class="section level3">
<h3>Reflectance of photosynthetic active vegetation</h3>
<ul>
<li><p>Vegetation produces a distinct spectral reflectance pattern due to its leaf and cell structure, its physiognomy, and complex stand structure</p></li>
<li><p>The reflectance of photosynthetically active vegetation is characterized by different factors in the VIS, nIR and SWIR:</p>
<ul>
<li><p>VIS - leaf pigments</p></li>
<li><p>nIR - cell structure</p></li>
<li><p>SWIR - water content</p></li>
</ul></li>
<li><p>Photosynthetically inactive plant parts differ considerably from active ones across different wavelength regions</p></li>
</ul>
<div class="figure">
<img src="fig/photo%20active%20veg%20reflectance.png" alt="" />
<p class="caption">Spectra of photosynthetic active vegetation; VIS (leaf pigments) in green, nIR (cell structure) in red and SWIR (water content) in blue</p>
</div>
<div id="single-leaf-reflectance" class="section level4">
<h4>Single leaf reflectance</h4>
<ul>
<li><p>The reflectance of a single leaf or needle is determined by various absorption, transmission and reflectance characteristics</p></li>
<li><p>Important characteristics are:</p>
<ul>
<li><p>The local reflectance maximum in the VIS (green peak) is small</p></li>
<li><p>The high reflectance in the nIR results in the so-called “red edge”</p></li>
<li><p>Thus, transmission in the nIR is very high, in the VIS very small</p></li>
<li><p>Lignin and cellulose, and particularly water, lead to high absorption, especially in the SWIR</p></li>
</ul></li>
</ul>
<div class="figure">
<img src="fig/single%20leaf%20reflectance.png" alt="" />
<p class="caption">Plant traits that influence vegetation spectra</p>
</div>
</div>
<div id="plant-stress" class="section level4">
<h4>Plant stress</h4>
<ul>
<li><p>According to the factors determining reflectance, there are also three determining damaging factors:</p>
<ul>
<li><p>Pigments</p></li>
<li><p>Cell structure</p></li>
<li><p>Cell water</p></li>
</ul></li>
<li><p>Destruction of chlorophyll causes a flattening of the C<sub>ab</sub>-specific absorption bands</p></li>
<li><p>Destruction of the cell structure causes a shift of the red edge towards the nIR and a flattening of the nIR-plateau</p></li>
<li><p>Both effects can be detected with remote sensing systems</p></li>
</ul>
<div class="figure">
<img src="fig/plant%20stress%20reflectance.png" alt="" />
<p class="caption">Spectra of vegetation with (green) and without (orange) plant stress (Source: Gausman 1974)</p>
</div>
</div>
<div id="water-stress" class="section level4">
<h4>Water stress</h4>
<ul>
<li><p>Dehydration of the cell leads to a flattening of absorption bands that depend on water content</p></li>
<li><p>Additionally, albedo increases considerably</p></li>
<li><p>Absorption bands of cellulose and lignin appear more clearly</p></li>
<li><p>Heavy water stress ultimately leads to a destruction of the cell structure and a decrease in plant metabolism</p></li>
</ul>
<div class="figure">
<img src="fig/water%20stress%20reflectance.png" alt="" />
<p class="caption">Spectra of vegetation with low (red), medium (green), high (blue) and very high (black) water stress</p>
</div>
</div>
<div id="multi-layered-leaf-reflectance" class="section level4">
<h4>Multi-layered leaf reflectance</h4>
<ul>
<li><p>The plant’s leaf organs produce complex reflectance patterns</p></li>
<li><p>In the VIS, one leaf layer is enough to absorb almost the entire radiation</p></li>
<li><p>In the nIR, complex radiation paths relate to multiple transmission</p></li>
</ul>
<div class="figure">
<img src="fig/multi%20layered%20leaf%20reflectance.png" alt="" />
<p class="caption">reflectance behaviour of wavelengths in the VIS (blue) and nIR (red) on plant leaf layers</p>
</div>
<ul>
<li><p>As the degree of transmission varies greatly in different wavelength regions, non-linear changes of the reflectance behaviour occur with an increasing number of leaf layers</p></li>
<li><p>A saturation of the reflectance in the nIR is – depending on the character of the respective leaves or needles – reached with 6 to 7 leaf layers</p></li>
<li><p>A commonly used measure for the number of leaf layers is the so called ‘leaf area index’ (LAI), measured in [m<sup>2</sup> • m<sup>-2</sup>]</p></li>
</ul>
</div>
<div id="plants-as-a-system" class="section level4">
<h4>Plants as a system</h4>
<ul>
<li><p>Plants do include photosynthetically inactive parts</p></li>
<li><p>Fruits, spines, flowers, bark, and dead parts of the plant affect the reflectance</p></li>
<li><p>Also structural factors affect the signal</p></li>
<li><p>Aircraft or satellite based methods detect an integral signal</p></li>
<li><p>In remote sensing, the analysis often focuses on the green parts of the plants only</p></li>
</ul>
<div class="figure">
<img src="fig/plants%20as%20system.png" alt="" />
<p class="caption">Reflectance spectra of a stack of leaves (green), the whole plant (blue) and bark (red)</p>
</div>
</div>
</div>
<div id="reflectance-of-soils" class="section level3">
<h3>Reflectance of soils</h3>
<div id="soil-moisture" class="section level4">
<h4>Soil moisture</h4>
<ul>
<li><p>Example: silty loam</p></li>
<li><p>Alteration of</p>
<ul>
<li><p>Form of the reflectance curve</p></li>
<li><p>Albedo</p></li>
<li><p>Absorption depth</p></li>
<li><p>FWHM (here: maximum absorption depth, similar to sensitivity function of sensors &gt; compare chapter on spectral resolution)</p></li>
</ul></li>
</ul>
<div class="figure">
<img src="fig/soil%20reflectance.png" alt="" />
<p class="caption">Reflectance spectra of soil with low and high water content (Source: Asrar 1989, adapted)</p>
</div>
</div>
<div id="organic-material-content" class="section level4">
<h4>Organic material content</h4>
<ul>
<li><p>Figure shows difference between low and very high organic content</p></li>
<li><p>With increasing portion of C<sub>org</sub></p>
<ul>
<li><p>The total reflectance decreases</p></li>
<li><p>Particularly the reflectance at 0.6 µm decreases</p></li>
<li><p>The depth of the water-dependent absorption bands decreases</p></li>
</ul></li>
</ul>
<div class="figure">
<img src="fig/OM%20content.png" alt="" />
<p class="caption">Reflectance spectra of soil with low and high organic material content</p>
</div>
</div>
</div>
<div id="reflectance-of-water" class="section level3">
<h3>Reflectance of water</h3>
<ul>
<li><p>Water, in its fluid phase, is an excellent absorber (especially in the infrared)</p></li>
<li><p>Clear water reflects less than 5 % in the VIS</p></li>
<li><p>In the nIR and SWIR, reflectance is close to 0</p></li>
<li><p>Accordingly, water is almost a black body in these wavelength regions</p></li>
<li><p>In its frozen form of ice, and particularly as snow, water is a good reflector</p></li>
</ul>
<div class="figure">
<img src="fig/water%20reflectance.png" alt="" />
<p class="caption">Reflectance spectra of pure water from simulation (dark blue) and river water from the Mosel (turquoise)</p>
</div>
</div>
</div>
<div id="usgs-spectral-characteristics-viewer" class="section level2">
<h2><a href="https://landsat.usgs.gov/spectral-characteristics-viewer">USGS spectral characteristics viewer</a></h2>
<hr />
</div>
</div>
<div id="optical-data-multi--hyperspectral" class="section level1">
<h1>Optical data (Multi- / Hyperspectral)</h1>
<div id="raster-formats-in-remote-sensing" class="section level2">
<h2>Raster formats in remote sensing</h2>
<ul>
<li><p>What you always need to know about your dataset for proper data handling:</p>
<ul>
<li><p>Spectral resolution (wavelengths, bands) <img src="fig/spectral%20resolution.png" /></p></li>
<li><p>Spatial resolution (pixel size, GSD), spatial resolution may differ for different bands in the same data set <img src="fig/spatial%20resolution.png" /></p></li>
<li><p>Rows + columns (height + width) <img src="fig/rows%20and%20columns.png" /></p></li>
<li><p>Radiometric resolution per band (data type) <img src="fig/radiometric%20resolution.png" /></p></li>
</ul></li>
<li><p>Band Interleaved by Pixel (BIP): for each band, gray values are stored in a pixel-wise manner on disk</p></li>
</ul>
<div class="figure">
<img src="fig/BIP.png" alt="" />
<p class="caption">BIP storage structure for three bands with n pixel</p>
</div>
<ul>
<li>Band Interleaved by Line (BIL): bands are stored line-wise</li>
</ul>
<div class="figure">
<img src="fig/BIL.png" alt="" />
<p class="caption">BIL storage structure for three bands with n pixel</p>
</div>
<ul>
<li>Band Sequential (BSQ): entire bands are stored on disk one after the other</li>
</ul>
<div class="figure">
<img src="fig/BSQ.png" alt="" />
<p class="caption">BSQ storage structure for three bands with n pixel</p>
</div>
<ul>
<li><p>Metadata or auxiliary data are either stored as part of an integrated data format (in the same file) or as an extra dataset accompanying the image data</p></li>
<li><p>Data types</p>
<ul>
<li><p>raster values are numbers that are stored in memory</p></li>
<li><p>image size = bands * lines * columns * bit width</p></li>
</ul></li>
</ul>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Data type</th>
<th align="left">Typical Bit Width</th>
<th align="left">Range</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">byte</td>
<td align="left">1 byte = 8 bit</td>
<td align="left">-127 to 127 (signed); 0 to 255 (unsigned)</td>
</tr>
<tr class="even">
<td align="left">integer</td>
<td align="left">2 byte = 16 bit</td>
<td align="left">-32768 to 32768 (signed); 0 to 65535 (unsigned)</td>
</tr>
<tr class="odd">
<td align="left">float</td>
<td align="left">4 byte = 32 bit</td>
<td align="left">-Inf to +inf, ‘single precision’; floating point number according to IEEE 754</td>
</tr>
<tr class="even">
<td align="left">double</td>
<td align="left">8 byte = 64 bit</td>
<td align="left">-Inf to +Inf, ‘double precision’</td>
</tr>
</tbody>
</table>
<ul>
<li><p>File types</p>
<ul>
<li><p>over 200 raster image formats (see <a href="https://gdal.org/drivers/raster/">gdal.org</a>)</p></li>
<li><p>Most common: ENVI, JPEG2000, GeoTiff</p></li>
</ul></li>
</ul>
<table>
<colgroup>
<col width="33%" />
<col width="33%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">Name</th>
<th align="left">Extension</th>
<th align="left">Description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">GeoTiff</td>
<td align="left">.tif, .tiff, .gtiff</td>
<td align="left">TIFF + geospatial reference</td>
</tr>
<tr class="even">
<td align="left">JPEG2000</td>
<td align="left">.jp2, .j2k</td>
<td align="left">used by many data providers; usually for integer values only</td>
</tr>
<tr class="odd">
<td align="left">ENVI</td>
<td align="left"><None>, .bsq, .bil, .bip, .dat</td>
<td align="left">generic, often used in imaging spectroscopy community; <em>Header file (.hdr) with meta data!</em></td>
</tr>
<tr class="even">
<td align="left">VRT</td>
<td align="left">.vrt</td>
<td align="left">GDAL Virtual Raster, XML text file</td>
</tr>
<tr class="odd">
<td align="left">HDF4, HDF5</td>
<td align="left">.hdf, .h4, .hdf4, .h5, .hdf5</td>
<td align="left">hierarchical data format, version 4 or 5; multi-resolution raster</td>
</tr>
<tr class="even">
<td align="left">netCDF</td>
<td align="left"></td>
<td align="left">Network Common Data Forat; multi-resolution raster</td>
</tr>
<tr class="odd">
<td align="left">SAVE</td>
<td align="left"></td>
<td align="left">Standard Archive Format for Europe e.g. Sentinel-1 and Sentinel-2</td>
</tr>
</tbody>
</table>
</div>
<div id="metadata" class="section level2">
<h2>Metadata</h2>
<ul>
<li>Metadata are additional information about the data and are commonly recorded during data acquisition, e.g. Exchangeable Image File Format (EXIF) in standard camera images</li>
</ul>
<div class="figure">
<img src="fig/metadata%20camera.png" alt="" />
<p class="caption">Metadata of a photograph</p>
</div>
<ul>
<li><p>Remote-sensing raster formats usually have their individual way to store metadata</p></li>
<li><p>ENVI format: binary file (.bsq) + header File (.hdr)</p>
<div class="figure">
<img src="fig/ENVI%20metadata.png" alt="" />
<p class="caption">Metadata in ENVI format as .hdr file</p>
</div></li>
<li><p>GeoTIFF: within .tiff image</p></li>
<li><p>GDAL: (.aux.pam) XML file to supplement existing metadata</p>
<div class="figure">
<img src="fig/GDAL%20metadata.png" alt="" />
<p class="caption">Metadata in GDAL format as XML file</p>
</div></li>
</ul>
</div>
<div id="hymap-data" class="section level2">
<h2>HyMap data</h2>
<div id="hymap-sensor-imaging-spectrometer" class="section level3">
<h3>HyMap-sensor: imaging spectrometer</h3>
<ul>
<li><p>Airborne hyperspectral sensor</p></li>
<li><p>Line scanner</p></li>
<li><p>Spectral range: 450 - 2500 nm</p></li>
<li><p>128 spectral channels</p></li>
<li><p>4 sensor modules á 32 bands</p></li>
<li><p>Geometric resolution: 3 - 10 m (depending on altitude)</p></li>
</ul>
<div class="figure">
<img src="fig/Hymap%20NASA.jpg" alt="" />
<p class="caption">HyMap sensor (Source: NASA)</p>
</div>
<div class="figure">
<img src="fig/Hymap%20Berlin.png" alt="" />
<p class="caption">HyMap data of Berlin</p>
</div>
<div class="figure">
<img src="fig/Beispielspektren%20Hymap.png" alt="" />
<p class="caption">HyMap sample spectra of “pure surface materials”</p>
</div>
</div>
</div>
<div id="exercise-3" class="section level2">
<h2>Exercise</h2>
<div id="dataset" class="section level3">
<h3>Dataset</h3>
<ul>
<li><p>Extract of Berlin-Brandenburg from <a href="http://doi.org/10.5880/enmap.2016.002">Berlin-Urban-Gradient dataset</a></p></li>
<li><p>Urban-land-gradient</p></li>
<li><p>Recorded 20th August 2009</p></li>
<li><p>111 of 128 spectral channels</p></li>
</ul>
<div class="figure">
<img src="fig/hymap%20berlin%202.png" alt="" />
<p class="caption">Dataset for the exercise</p>
</div>
</div>
<div id="discussion-of-data-type-and-structure-on-the-basis-of-the-hymap-image" class="section level3">
<h3>Discussion of data type and structure on the basis of the HyMap image</h3>
<ul>
<li><p>Open the header file (metadata) of the HyMap image on O:/ (…/S05/envi_format/HyMap_BB_subset.hdr) in Notepad++</p></li>
<li><p>What do you find out about the following data properties of the image?</p>
<ul>
<li><p>Dimension in x and y</p></li>
<li><p>Spatial resolution</p></li>
<li><p>Spectral resolution</p></li>
<li><p>Radiometric resolution</p></li>
<li><p>Data type</p></li>
<li><p>Interleave</p></li>
</ul></li>
<li><p>What additional data properties may be derived from the metadata?</p></li>
</ul>
<p><em>Upload on Moodle?</em></p>
<hr />
</div>
</div>
</div>
<div id="enmap-box-for-qgis" class="section level1">
<h1>EnMAP Box for QGIS</h1>
<div id="introduction" class="section level2">
<h2>Introduction</h2>
<ul>
<li><p>Freely available and platform-independent QGIS plugin</p></li>
<li><p>Developed by HU Berlin´s Earth Observation Lab for the hyperspectral satellite sensor EnMAP</p></li>
</ul>
<div class="figure">
<img src="fig/EnMAP%20intro.png" alt="" />
<p class="caption">Interface of the EnMAP plugin in QGIS</p>
</div>
</div>
<div id="installation" class="section level2">
<h2>Installation</h2>
<ul>
<li><p><a href="www.enmap.org">EnMAP-Box Portal</a></p></li>
<li><p><a href="enmap-box.readthedocs.io/en/latest/">Dokumentation</a></p></li>
<li><p><a href="enmap-box.readthedocs.io/en/latest/usr_section/usr_installation.html">Installation</a></p></li>
</ul>
<div class="figure">
<img src="fig/EnMAP%20installation.png" alt="" />
<p class="caption">EnMAP-Box documentation (see <a href="enmap-box.readthedocs.io/en/latest/">here</a>)</p>
</div>
</div>
<div id="exercise-4" class="section level2">
<h2>Exercise</h2>
<div id="part-1-get-to-know-the-enmap-box-and-key-features" class="section level3">
<h3>Part 1: Get to know the EnMAP-Box and key features</h3>
<ul>
<li>Menu structure, opening and portraying images, zoom in and out, selection of bands, contrast stretching</li>
</ul>
<p><img src="fig/enmap%20anhang%201.png" /></p>
<p><img src="fig/enmap%20anhang%202.png" /></p>
<p><img src="fig/enmap%20anhang%203.png" /></p>
<ul>
<li>Visualizing and collecting image spectra, editing of attribute information, colouring of spectra, saving spectral library</li>
</ul>
<p><img src="fig/enmap%20anhang%204.png" /></p>
<p><img src="fig/enmap%20anhang%205.png" /></p>
<p><img src="fig/enmap%20anhang%206.png" /></p>
<p><img src="fig/enmap%20anhang%207.png" /></p>
<p><img src="fig/enmap%20anhang%208.png" /></p>
</div>
<div id="part-2-creating-a-specral-library" class="section level3">
<h3>Part 2: Creating a specral library</h3>
<ul>
<li><p>Collect 5 reflectance spectra in the HyMap image (HyMAP_BB_Subset.bsq) for following (pure) materials/surfaces:</p>
<ul>
<li><p>Building</p></li>
<li><p>Sealed (non build-up)</p></li>
<li><p>Grass</p></li>
<li><p>Tree</p></li>
<li><p>Soil</p></li>
<li><p>Water</p></li>
</ul></li>
<li><p>Create two attribute columns (‘class’, ‘description’) and fill them in. The first serves as an index for the assignment of the six classes (i.e. ‘building’), the latter should include a detailed description of the material/surface (i.e. ‘red tiles’)</p></li>
<li><p>Colour the spectra appropriately with regard to the classes</p></li>
<li><p>Save the collected spectra via the Spectral Library Viewer as a Spectral Library file (SLI)</p></li>
<li><p>Export the image of the coloured spectra as a PNG file and create a PDF file (including a legend)</p></li>
<li><p><strong>Assignment:</strong> Put the Spectral Library (all files) and the PDF together in a ZIP file and upload the ZIP file on Moodle.</p></li>
</ul>
<hr />
</div>
</div>
</div>
<div id="data-acquisition" class="section level1">
<h1>Data acquisition</h1>
<div id="provider-vielleicht-eher-sentinel-2" class="section level2">
<h2>Provider (vielleicht eher ‘Sentinel 2’?)</h2>
<ul>
<li><p>COPERNICUS program features a pair of optical Earth observing satellites: Sentinel-2A and -2B Launched in June 2015 (Sentinel-2A), and March 2017 (Sentinel-2B)</p></li>
<li><p>Revisit time 5 days with both satellites, 290km swath width</p></li>
<li><p>Sensor: Multispectral Imager (MSI), 13 spectral bands, partly resembling Landsat</p></li>
<li><p>Spatial resolution of 10 – 60 m, depending on spectral bands, e.g. nIR bands 8 (10m) and 8a (20m).</p></li>
</ul>
<div class="figure">
<img src="fig/Sentinel2_bands.png" alt="" />
<p class="caption">Sentinel 2 vs Landsat 7 and 8</p>
</div>
</div>
<div id="sentinel-2" class="section level2">
<h2>Sentinel 2</h2>
<ul>
<li><p>Sentinel-2 products are delivered in various processing levels.</p>
<ul>
<li><p>Level 1B: Top-of-atmosphere radiance values in sensor geometry</p></li>
<li><p>Level 1C: Top-of-atmosphere reflectance in cartographic geometry</p></li>
<li><p>Level 2A: Bottom-of-atmosphere reflectance in cartographic geometry</p></li>
</ul></li>
<li><p>L1C and L2A products are partitioned in “granules”: 100x100 km2 UTM/WGS84 projection</p></li>
</ul>
<div class="figure">
<img src="fig/Sentinel2_granules.png" alt="" />
<p class="caption">Sentinel 2 Granules</p>
</div>
</div>
<div id="acquiring-sentinel-2-data" class="section level2">
<h2>Acquiring Sentinel-2 data</h2>
<ul>
<li><p>Create a User-Account at the <a href="https://scihub.copernicus.eu/dhus/#/home">Copernicus Open Access Hub</a></p></li>
<li><p>Search for images with these specifications:</p>
<pre><code>Date: XXX

Sensor: Sentinel-2

Producttype: Surface Reflectance (Level 2A, S2MSI2A) 

Cloud cover: max. 20%

Region: greater area of Berlin</code></pre></li>
<li><p>How many images are available for these specifications? download this image:</p></li>
</ul>
<p>Granule: XXX</p>
<p>Date: XXX</p>
</div>
<div id="pre-processing" class="section level2">
<h2>Pre-processing</h2>
<ul>
<li><p>The downloaded Sentinel-2 image is a .zip-file</p></li>
<li><p>Unzip it and delete the .zip-file</p></li>
<li><p>Take a look at your files</p></li>
</ul>
<div class="figure">
<img src="fig/Sentinel2_filestructure.png" alt="" />
<p class="caption">Sentinel 2 filestructure</p>
</div>
<div id="visualizing-the-sentinel-2-image-with-virtual-raster-builder-in-the-enmap-box" class="section level3">
<h3>Visualizing the Sentinel-2 image with ‘Virtual Raster Builder’ in the EnMAP-Box</h3>
<ul>
<li><p>Goal: the single bands in JPEG2000-format (.jp2) will be merged into a single Multiband-Image in ENVI-format</p></li>
<li><p>Open QGIS and install the ‘Virtual Raster Builder’ plugin</p></li>
<li><p>Open the EnMAP-Box and load ‘S2_Subset_Berlin.shp’ into Data Sources. The shapefile will be used to clip the Sentinel-2 scene to a smaller extend</p></li>
<li><p>Open ‘Virtual Raster Builder’ from the EnMAP-Box. Create a Multiband-Image with the following specifications:</p>
<ul>
<li>Spectral Bands: B2, B3, B4, B5, B6, B7, B8a, B11, B12</li>
<li>Spatial Resolution: 20m (from the ‘R20M’ file)</li>
<li>Spatial Extend: according to the shapefile</li>
<li>Format: ENVI</li>
</ul></li>
</ul>
</div>
<div id="create-vrt-in-den-folien-vom-vorjahr-soll-man-ein-envi-erstellen" class="section level3">
<h3>Create VRT (in den Folien vom Vorjahr soll man ein ENVI erstellen…)</h3>
</div>
<div id="amend-metadata" class="section level3">
<h3>Amend metadata</h3>
<ul>
<li><p>Visualize the Multiband Sentinel-2 Image in the EnMAP-Box with the band combination R = 8a (nIR), G = 4 (red), B = 3 (green).</p></li>
<li><p>Take a look at the .hdr file. Which metainformation is missing ?</p></li>
<li><p>Delete the edited Sentinel-2 image from the Data Source Panel. Also delete the .aux-file and .vrt-file from apple finder / windows explorer</p></li>
<li><p>Add the wavelengths of the single bands and the unit for the wavelengths in the .hdr file (metadata). Research the unit and the extend of the applied Sentinel-2 wavelengths.</p></li>
<li><p>Reload the Sentinel-2 image into the Data Source Panel. The wavelengths should now be displayed on the x-axis when visualizing at image spectra. Now you can compare spectra from different Sensors.</p></li>
</ul>
</div>
<div id="compare-s2-hymap" class="section level3">
<h3>Compare S2 &amp; HyMap</h3>
<p>part 1 - spatial resolution</p>
<ul>
<li><p>Visualize the Sentinel-2 image and the HyMap image from last week in a RGB bandcombination that is suited for comparison. In order to do that open the images in two separate MapViews and link them (spatialy + zoom)</p></li>
<li><p>Compare both images visually and describe the effects on image details due to the different spatial resolution (20 m vs. 3.6 m).</p></li>
<li><p>Choose one surface as example for each of these classes: impervious- build up, impervious- not build up, grass/lawn, trees, soil, water.</p></li>
<li><p>Take a screenshot from each example.</p></li>
</ul>
</div>
<div id="compare-s2-hymap-1" class="section level3">
<h3>Compare S2 &amp; HyMap</h3>
<p>part 2 - spectral resolution</p>
<ul>
<li><p>Restart QGIS and EnMAP-Box. Display the Sentinel-2 image and the HyMap image in a suited RGB band combination (Wozu ist der Neustart notwendig ?)</p></li>
<li><p>Visualize spectra of the same surface in the Sentinel-2 and in the HyMAP image (9 vs. 111 spectral bands).</p></li>
<li><p>Try to choose a ‘pure’ surface (no mixtures) which is also stable in temporal terms (unchanged over time).</p></li>
<li><p>Do this for the classes mentioned above and add a description and a screenshot.</p></li>
<li><p><strong>Assignment:</strong> please upload the comparison (spatial and spectral) of the Sentinel-2 and the HyMap image as pdf to moodle.</p></li>
</ul>
<div class="figure">
<img src="fig/homework_example.png" alt="" />
<p class="caption">assignment example</p>
</div>
<hr />
</div>
</div>
</div>
<div id="vegetation-properties-spectral-indices" class="section level1">
<h1>Vegetation properties &amp; spectral indices</h1>
<p><img src="fig/spectral_resolution_comparison.png" /></p>
<div id="normalized-difference-vegetation-index-ndvi" class="section level2">
<h2>Normalized Difference Vegetation Index (NDVI)</h2>
<ul>
<li><p>The derivation of vegetation indicies or similar variables is based on feature extraction</p></li>
<li><p>NDVI the one of the most common vegeation indicies</p></li>
<li><p>It represents the difference between red and nIR reflectance. This difference is strongly prounounced with photosynthetically active vegetation. (Sollte man wahrscheinlich nochmal umschreiben)</p></li>
<li><p>The NDVI displayes this difference within a normalized value (between -1 and 1) for each pixel.</p></li>
</ul>
<p><span class="math display">\[NDVI = (ρnIR- ρred) / (ρnIR+ ρred)\]</span></p>
<ul>
<li><p>e.g. for Sentinel-2 data use reflectance from bands nIR = 865 nm and red = 665 nm for calculating the NDVI.</p></li>
<li><p>The calculation of the NDVI results in a singleband grayscale image which can be saved as a new file.</p></li>
</ul>
</div>
<div id="part-1-calculating-the-normalized-difference-vegetation-index-ndvi-for-sentinel-2-data" class="section level2">
<h2>Part 1: Calculating the Normalized Difference Vegetation Index (NDVI) for Sentinel-2 data</h2>
<ul>
<li><p>Open the EnMAP-Box and display the Sentinel-2 image (20 m, 9 spectral bands, subset Berlin, DATE: XXX) from last week in a RGB bandcombination of your choice.</p></li>
<li><p>via ‘Applications’ start the ‘imageMath Calculator’ and calculate the NDVI.</p>
<pre><code>1. specify the the input image    -&gt; S2_20m = [select Sentinel-2 image]

2. define bands nIR [865 nm] and  -&gt; red    = float32(S2_20m[2])
   red [665 nm] as variable       -&gt; nIR    = float32(S2_20m[6])  

3. provide the NDVI formular      -&gt; NDVI   = (nIR - red)/(nIR + red)

4. specify the output image       -&gt; NDVI   = [filepath and name of the NDVI image]</code></pre></li>
<li><p>Run the function and open the singleband grayscale image in a new Map View</p></li>
</ul>
</div>
<div id="part-2-discussion-of-the-ndvi" class="section level2">
<h2>Part 2: Discussion of the NDVI</h2>
<ul>
<li><p>Take a look at the NDVI-Pixelvalues and create a NDVI image-histogram (Tools ‘ImageStatistics’)</p></li>
<li><p>Which value range take pixels in the NDVI image ?</p></li>
<li><p>Note a representative NDVI value for these surfaces: water, asphalt, decidous forest, open soil.</p></li>
<li><p>Also, note the corresponding refectance values in the red and nIR band from the original image. How can the values be explained, concerning the spectral properties of the surfaces?</p></li>
<li><p>Under what cirumstances (relation red to nIR) can NDVI values of 0; 1; -1 be created (in theory)? Please note one pair of values (red and nIR) for each of these NDVI values.</p></li>
</ul>
</div>
<div id="part-3-visualizing-of-ndvi-classes" class="section level2">
<h2>part 3: Visualizing of NDVI classes</h2>
<ul>
<li>classify the NDVI image through the style menu within the layer properties. Try to display three surfaces: water, impervious/soil and vegetation.</li>
</ul>
<p><img src="fig/NDVI_classification_properties.png" /></p>
</div>
<div id="assignment-part-1-correlation-of-ndvi-and-imperviousness" class="section level2">
<h2>Assignment part 1: Correlation of NDVI and Imperviousness</h2>
<ul>
<li><p>Open the image ‘copernius_imperviousness_2015_berlin.bsq’ in a second Map Window and link it to the NDVI image.</p></li>
<li><p>Inform yourself about the origin of the layer: (<a href="https://land.copernicus.eu/pan-european/high-resolution-layers/imperviousness" class="uri">https://land.copernicus.eu/pan-european/high-resolution-layers/imperviousness</a>)</p></li>
<li><p>Gather NDVI values of 20 surfaces (zu viele, 10 würden reichen) in a table. Make sure to cover a wide range of values (e.g. -0.2 to 1).</p></li>
<li><p>Create a scatterplot that shows the correlation between NDVI and imperviousness. Use a software of your choice; prefarably R.</p></li>
<li><p>Describe and discuss the scatterplot in bullitpoints.</p></li>
</ul>
</div>
<div id="assignment-part-2-temporal-variation-of-the-ndvi" class="section level2">
<h2>Assignment part 2: Temporal Variation of the NDVI</h2>
<ul>
<li><p>Calculate another NDVI image from the provided Sentinel-2 image taken in winter (date XXX). Use the ‘imageMath Calculator’.</p>
<p>-&gt; Sentinel_2_T33UUU_20190216_20m_9bands_subset_berlin.bsq</p></li>
<li><p>Then calculate a difference-image for summer and winter NDVI.</p>
<p>-&gt; NDVI 27. Juli 2019 – NDVI 16. Februar 2019 DATES: XXX</p></li>
<li><p>Create the histogram of the difference-image and briefly discuss the distribution</p></li>
<li><p>Display the difference-image in a suited Visualisation; e.g. discrete classes, colour gradient from high negative to high positive differences.</p></li>
<li><p>Discuss the resulting Map in bullitpoints concerning these surfaces: impervious (buildings, and non build-up), agriculture, forest, water.</p></li>
<li><p><strong>Assignment:</strong> Please upload the results of the comparison of NDVI and imperviousness (part 1, plot and discussion) and the temporal variation of NDVI (part 2, histogram, map + legend, discussion) as pdf to moodle.</p></li>
</ul>
<p>Berechnen Sie das Histogramm des NDVI-Differenzbildes und diskutieren Sie die Verteilung stichpunktartig.</p>
<p>Stellen Sie das NDVI-Differenzbild mit einer sinnvollen Visualisierung dar, z.B. einer diskrete Klassenunterteilung, oder einem Farbgradienten von hohen negativen bis zu hohen positiven Differenzen. Diskutieren Sie die Karte stichpunktartig bzgl. folgender Oberflächen: Versiegelte Flächen (Gebäude &amp; unbebaut), Landwirtschaft, Wald, Wasser Bitte laden Sie die Ergebnisse des Vergleiches NDVI und Versieglung (Teil 1, Plot, Diskussion) und der temporalen Variation des NDVI (Teil 2; Histogramm, Karte + Legende, Diskussion) als PDF in Moodle hoch.</p>
<hr />
</div>
</div>
<div id="image-classification" class="section level1">
<h1>Image classification</h1>
<hr />
</div>
<div id="random-forest" class="section level1">
<h1>Random forest</h1>
<hr />
</div>
<div id="accuracy-assessment" class="section level1">
<h1>Accuracy assessment</h1>
<hr />
</div>
<div id="sentinel-2-time-series" class="section level1">
<h1>Sentinel-2 time series</h1>
<hr />
</div>
<div id="modis-time-series" class="section level1">
<h1>MODIS time series</h1>
<hr />
</div>
<div id="case-studies-change-detection" class="section level1">
<h1>Case studies: Change detection</h1>
<hr />
</div>

    </div>
    <div class="col-xs-2">
        </div>
  </div>
  </div>
  </div>
  <div class="row">
    </div>
  </div>

<script>
$(document).ready(function () {
  // add bootstrap table styles to pandoc tables
  $('tr.header').parent('thead').parent('table').addClass('table table-striped table-hover');

    var images = $('.pages img');
  images.filter(function() {
      if ($(this).parent().attr("class") == "figure") {
          return(false)
      } else {
          return(true);
      }
  }).wrap("<div class='figure'></div>");
  images.addClass("image-thumb").wrap("<div class='panel-body'></div>");
  $('.figure p.caption').wrap("<div class='panel-footer'></div>");
  $('.figure').addClass('panel panel-default');
  
    $('.pages img')
 	  .addClass("image-lb");
  $('.pages').magnificPopup({
	      type:'image',
	      closeOnContentClick: false,
	      closeBtnInside: false,
        delegate: 'img',
	      gallery: {enabled: false },
          removalDelay: 500,
          callbacks: {
              beforeOpen: function() {
                // just a hack that adds mfp-anim class to markup
                this.st.image.markup = this.st.image.markup.replace('mfp-figure', 'mfp-figure mfp-with-anim');
              }
          },
          mainClass: 'mfp-move-from-top',
	      image: {
	        verticalFit: true,
            titleSrc: 'alt'
	      }
 	    });
 	
    
    $('#toc ul li').first().addClass("active");
    $('#toc ul li').attr("data-target", function() {
        return($(this).children("a").attr("href"));
    })
    $('body .section.level1').first().addClass("active");
    $('body .section.level1').not('.active').hide();
    
    $('#toc a[href*="#"]').click(function() {

      var id = $(this).attr("href");
      if (id === "#") return;
      if (id.substring(0, 8) === "#dyntab-") return;
      toggle_page(id);

      // Menu
      var menu_entry = $(".menu li[data-target='"+id+"']");
      menu_entry.addClass("active");
      $(".menu li").not(menu_entry).removeClass("active"); 
      

    });

    function toggle_page(id) {
      $(".page").not(page).removeClass("active").hide();
      window.page = id;
      var page = $(window.page);
      window.location.hash = window.page;
      //$(this).addClass("active");

      page.show();

      var totop = setInterval(function () {
        $(".pages").animate({scrollTop: 0}, 0);
      }, 10);

      setTimeout(function () {
        page.addClass("active");
        setTimeout(function () {
          clearInterval(totop);
        }, 1000);
      }, 100);

      window.dispatchEvent(new Event('resize'));

    }


    $(".menu li").click(function () {

      toggle_page($(this).data("target"));

      // Menu
      if (!$(this).data("target")) return;
      if ($(this).is(".active")) return;
      $(".menu li").not($(this)).removeClass("active");
      $(this).addClass("active");

    });
  
    


    window.page = window.location.hash;
    if (window.page != "") {
      $(".menu").find("li[data-target=" + window.page + "]").trigger("click");
    }

    /* init material bootstrap js */
    $.material.init();
});
</script>




<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
